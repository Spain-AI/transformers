{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Training Transformers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://i0.wp.com/wallur.com/wp-content/uploads/2016/12/transformers-background-1.jpg?w=1920\">\n",
    "<div align=\"right\"><a href=http://wallur.com/wallpaper/36471>Image source</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this notebook we will tackle the task of detecting toxic comments in social media, making use of a pre-trained Transformer-based language model to do so. The data we will use are a simplified version of the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) dataset, where comments with several toxicity labels have been simplified to just one label (multilabel to multiclass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "To avoid missing packages and compatibility issues you should run this notebook under the environment defined in the accompanying environment file, or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your are running this notebook in Google Colaboratory, you will need to install the transformers library by uncommenting and running the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==2.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set a random seed so experiments are reproducible across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided as two separate files, one with texts for training the model and another one for testing. Both files are available in compressed form under the *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/toxic_multiclass_train.csv.zip\", index_col=\"id\")\n",
    "test = pd.read_csv(\"data/toxic_multiclass_test.csv.zip\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have loaded the data properly, you should be able to visualize the first rows of each data set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data files include a column *comment_text* with the text we must classify, and an additional columns with the kind of toxicity that is presents in a comment: *toxic*, *severe_toxic*, *obscene*, *threat*, *insult* and *identity_hate*, or *normal* if the text contains no toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow faster experimenting, we will only use a portion of the data. Note that reducing the training data will result in worse model performance, and reducing the test data will result in a poorer estimate of the performance of the model. If you want to obtain the best results with the best confidence, do not run the following cell. But be prepared for a very, VERY long training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Training patterns before reduction: {len(train)}\")\n",
    "train = train.sample(int(len(train)/100), random_state=12345)\n",
    "print(f\"Training patterns after reduction:  {len(train)}\")\n",
    "\n",
    "print(f\"Test patterns before reduction: {len(test)}\")\n",
    "test = test.sample(int(len(test)/100), random_state=12345)\n",
    "print(f\"Test patterns after reduction:  {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = train[\"comment_text\"].values\n",
    "X_test = test[\"comment_text\"].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train[\"toxicity\"].values)\n",
    "y_test = label_encoder.transform(test[\"toxicity\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient library to make use of Transformer-based language models is... [Transformers](https://github.com/huggingface/transformers)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/transformers_logo_name.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers provides implementations of many language models like BERT, GPT-2 and many more. It also allows to make use of pre-trained versions of these models, thus saving a lot of time when solving practical problems.\n",
    "\n",
    "Let's start by importing an AutoConfig object, which allows us to specify the configuration details for a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the provided models, in this notebook we will make use of DistilBERT, a distilled version of BERT that can obtain good accuracies while keeping the model size small. We will use the configuration to tell Transformers we want to use a pretrained version of DistilBERT, trained on a dataset of uncased data, since case might not be important for the problem at hand. We also need to specify that we will use this pre-trained model to solve a classification problem with a specific number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'distilbert-base-uncased'\n",
    "num_labels = len(set(y_train))\n",
    "config = AutoConfig.from_pretrained(pretrained_model, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the resultant configuration, which contains all the model parameters, like dropout rates, embeddings sizes, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in a language model pipeline is to tokenize the data. We can do so using an AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will load a particular tokenizer: the one used for training DistilBERT. This tokenizer is pre-trained with an uncased dataset, following the pattern we specifyied in the configuration above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"A long trip to Mordor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most BERT models use a word-pieces tokenizer, dividing text into tokens that might represent a whole word, or a part of a word if the word is not common in the language. Also, since we are using an uncased model, the tokenizer maps all words to lower case.\n",
    "\n",
    "Equivalently, we can also ask the tokenizer to transform the text to a list of dictionary ids, plus other lists of indexes required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"A long trip to Mordor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding also adds the special tokens `[CLS]` and `[SEP]` required by BERT models at the beginning and end of each text. We can check that out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_id in tokenizer.encode(\"A long trip to Mordor\"):\n",
    "    print(f'{token_id} -> {tokenizer.decode([token_id])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to tokenize more than one text we can use the `batch_encode_plus` function. We can configure this function to make sure that every encoded text has the same length, which will we useful when working in batches on the GPU. In  the following example we will use a common length of 10, which manages to cover all tokens in every one of these sample texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"A long trip to Mordor\", \n",
    "    \"Our mind a sea\",\n",
    "    \"Mabuka is the end of light\"\n",
    "]\n",
    "\n",
    "tokenizer.batch_encode_plus(texts, max_length=10, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_encode_plus` returns a dictionary with three entries:\n",
    "\n",
    "* `input_ids`: the ids of the tokens encoding each of the texts.\n",
    "* `attention_mask`: 0/1 indicators telling whether the attention layers should consider this token in the mixings or not. Padding tokens always get a 0 value in the mask.\n",
    "* (optional) `token_type_ids`: for language models that learn with pairs of sentences, 0/1 indicators telling the sentence to which each token belongs.\n",
    "\n",
    "All the returned elements will be necessary as inputs to the language model. Since DistilBERT does not learn from pairs of sentences, the `token_type_ids` entry is not returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the `batch_encode_plus` function to produce Tensorflow or Pytorch tensors instead of python lists. For instance, to obtain Pytorch tensors we will do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_encode_plus(texts, max_length=10, pad_to_max_length=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned structure is the same, but now each entry is a Pytorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what would be the ideal maximum length for encoding our texts? BERT accepts inputs texts as long as 512 tokens, but using always this maximum length will result in slow training times. We can try tokenizing all texts without length limitation and study the distribution of text lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tokenizer.batch_encode_plus(X_train)\n",
    "lenghts = [len(x) for x in encoded[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the numpy function `quantile` to obtain a length in which 90% of the documents can fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "maxlength = int(np.quantile(lenghts, 0.9))\n",
    "maxlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this maximum length later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to explore the DistilBERT model. Let's load the pre-trained version of DistilBERT using an AutoModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "distilbert = AutoModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pre-trained version contains the \"body\" of the model, which can receive a sequence of tokens and produce the \"contextualized\" embeddings for each one of those tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/bert-encoders-input.png\">\n",
    "<div align=\"right\">Image credit: <a href=\"http://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this with a batch of one text of the training data, but remember we first need to transform it through the tokenizer and obtain Pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tokenizer.batch_encode_plus(X_train[0:1], max_length=40, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can input the tensor into the DistilBERT model. A convenient way to input all three tensors into the model is using the unpacking operator `**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = distilbert(**sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformers model always returns a tuple which might contain several pieces of information. In the case of DistilBERT, only a single object is returned, which is a pytorch tensor containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = outputs[0]\n",
    "print(f\"Input tensor shape {sample['input_ids'].shape}\")\n",
    "print(f\"Input tensor values {sample['input_ids']}\")\n",
    "print(f\"DistilBERT embeddings shape {embeddings.shape}\")\n",
    "print(f\"DistilBERT embeddings values {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT returns an embedding vector of 768 numbers for each input token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is tempting to use these embeddings as features for the toxic classification task, this approach does not generally give good results. Instead, it is advisable to add a classification \"head\" to the model, growing out of the embedding produced for the `[CLS]` special token, and fine-tune the whole model to the task through back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/bert-classifier.png\">\n",
    "<div align=\"right\">Image credit: <a href=\"http://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformers library can prepare all of this for us, by loading a version of DistilBERT with a Sequence Classification head. We will provide the configuration we prepared above, so the the classification head produces as many outputs as classes in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "distilbert_classification = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a Transformers model is not as simple as fitting scikit-learn or Keras model: we will need to provide all the details on how to batch the data, as well as other details on the training procedure. To easen this task, we will first prepare some useful functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fine-tuning a language model on the CPU is not a good idea. We will be better off using GPUs. To do so, we first need to identify the computing device. The code below checks if GPUs are available in the system, and if so, prepares a Torch device to send the calculations there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above should print `device(type='cuda')` if a GPU was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if your machine has several GPUs, all of them will be used for training. To constrain this notebook to use a specific GPU, run jupyter notebook as \n",
    "\n",
    "   `CUDA_VISIBLE_DEVICES=X jupyter notebook`\n",
    "   \n",
    "where `X` is the number of the GPU you want to use. You can check your available GPUs and their usage with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset used in Transformers must follow one contraint: the dataset must be an iterable of samples. But what is a sample? A text together with the outputs we expect our model to generate for it. In our classification problem each sample will be a text together with its class, so a natural way to organize a sample is the forma a tuple (text, class). We will prepare out training and test data in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train, y_train))\n",
    "eval_dataset = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will need a <b>collate class</b> that receives an part of a dataset (iterable of samples), and performs all the tokenization and encoding procedure to obtain a Torch tensor in GPU. This class should inherit from the Transformers `DataCollator` class, and implement a `collate_batch` method that receives an iterable of samples and returns an encoded batch. Here we provide an implementation for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollator\n",
    "\n",
    "class TextClassificationCollator(DataCollator):\n",
    "    \"\"\"Data collator for a text classification problem\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        \"\"\"Initializes the collator with a tokenizer and a maximum document length (in tokens)\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Transforms an iterable of texts into a dictionary of model input tensors, stored in the GPU\"\"\"\n",
    "        # Tokenize and encode texts as tensors, with maximum length\n",
    "        tensors = self.tokenizer.batch_encode_plus(\n",
    "            texts, \n",
    "            max_length=self.max_length, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Move tensors to GPU\n",
    "        for key in tensors:\n",
    "            tensors[key] = tensors[key].to(device)\n",
    "        return tensors\n",
    "    \n",
    "    def collate_batch(self, patterns):\n",
    "        \"\"\"Collate a batch of patterns\n",
    "        \n",
    "        Arguments:\n",
    "            - patterns: iterable of tuples in the form (text, class)\n",
    "            \n",
    "        Output: dictionary of torch tensors ready for model input\n",
    "        \"\"\"\n",
    "        # Split texts and classes from the input list of tuples\n",
    "        train_idx, targets = zip(*patterns)\n",
    "        # Encode inputs\n",
    "        input_tensors = self.encode_texts(train_idx)\n",
    "        # Transform class labels to a tensor in GPU\n",
    "        Y = torch.tensor(targets).long().to(device)\n",
    "        # Return batch as a dictionary wikth all the inputs tensors and the labels\n",
    "        batch = {**input_tensors, \"labels\": Y}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the collator class defined, we will create an instance for the particular tokenizer and maximum sequence length we have chosen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = TextClassificationCollator(tokenizer, maxlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating a `TrainingArguments` object. This object allows us to specify the training procedure details. For this notebook we will use a batch size of 32, which should fit into a small GPU. We will also train the model only 1 epoch over the training data, to allow us to check the results quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/toxic_model\",  # Folder in which to save the trained model\n",
    "    overwrite_output_dir=True,  # Whether to overwrite previous models found in the output folder\n",
    "    per_gpu_train_batch_size=16,  # batch size during training\n",
    "    per_gpu_eval_batch_size=128,  # batch size during evaluation (prediction)\n",
    "    num_train_epochs=1,  # Model training epochs\n",
    "    logging_steps=25,  # After how many training steps (batches) a log message showing progress will be printed\n",
    "    save_steps=25  # After how many training steps (batches) the model will be checkpointed to disk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the preparation, is to create a `Trainer` object. This is the object that performs the actual training, and will need to receive all the information we prepared above, which is:\n",
    "\n",
    "* The model to be fine-tuned\n",
    "* The `TrainingArguments` object we prepared above\n",
    "* Training and evaluation datasets\n",
    "* The DataCollator object that will batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=distilbert_classification,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the work above is done, training is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we can obtain the model predictions with the `predict` method of the `Trainer` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in a `ForSequenceClassification` model each prediction is a list of **unnormalized** probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain actual probabilities we need to apply a `softmax` function, which enforces each probability to take a value in the range `[0, 1]`, and also that all probabilities sum up to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "probs = softmax(preds.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check now that indeed the number we have look like probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these probabilities to compute any standard metric from scikit-learn. For instance, ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC score\", roc_auc_score(y_test, probs, multi_class='ovr'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
