{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "# Training Transformers models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "<img src=\"https://i0.wp.com/wallur.com/wp-content/uploads/2016/12/transformers-background-1.jpg?w=1920\">\n",
    "<div align=\"right\"><a href=http://wallur.com/wallpaper/36471>Image source</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "In this notebook we will tackle the task of detecting toxic comments in social media, making use of a pre-trained Transformer-based language model to do so. The data we will use are a simplified version of the [Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) dataset, where comments with several toxicity labels have been simplified to just one label (multilabel to multiclass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": false,
     "solution": false
    }
   },
   "source": [
    "To avoid missing packages and compatibility issues you should run this notebook under the environment defined in the accompanying environment file, or make use of [Google Colaboratory](https://colab.research.google.com/). If you use Colaboratory make sure to [activate GPU support](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your are running this notebook in Google Colaboratory, you will need to install the transformers library by uncommenting and running the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers==2.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us set a random seed so experiments are reproducible across runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is provided as two separate files, one with texts for training the model and another one for testing. Both files are available in compressed form under the *data* folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train = pd.read_csv(\"data/toxic_multiclass_train.csv.zip\", index_col=\"id\")\n",
    "test = pd.read_csv(\"data/toxic_multiclass_test.csv.zip\", index_col=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have loaded the data properly, you should be able to visualize the first rows of each data set as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e0fdfd98c66fb643</th>\n",
       "      <td>\"\\n\\n Huggle not working \\n\\nHi Gurch. There i...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864753b5fb6c9a3</th>\n",
       "      <td>Mossad actually.  I know where you live.</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ce1db53fb22d399c</th>\n",
       "      <td>REDIRECT Talk:UFC Fight Night: Belfort vs. Hen...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fed4f08d59399398</th>\n",
       "      <td>\"\\n\\nUPA IRC\\nWhat about 19:00 UTC?  e  | ταλκ \"</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>06e7f93938ad9e72</th>\n",
       "      <td>\"\\nI've re-added your information, together wi...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a5e851879fdd674</th>\n",
       "      <td>\"\\nI'm not an elitist, I'm just spreading the ...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ff39db4975a78363</th>\n",
       "      <td>\"\\n\\nIt is not listed on this European list as...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73cc03c5e157ce86</th>\n",
       "      <td>You made a mistake you ass.</td>\n",
       "      <td>obscene</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ca0891e20b7bbd66</th>\n",
       "      <td>Lol dynamic IP. Just you try to stop me! 82.13...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b890cc6153e51480</th>\n",
       "      <td>\"Thanks for trying to fix Neil Steinberg. I ju...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text toxicity\n",
       "id                                                                          \n",
       "e0fdfd98c66fb643  \"\\n\\n Huggle not working \\n\\nHi Gurch. There i...   normal\n",
       "1864753b5fb6c9a3           Mossad actually.  I know where you live.   normal\n",
       "ce1db53fb22d399c  REDIRECT Talk:UFC Fight Night: Belfort vs. Hen...   normal\n",
       "fed4f08d59399398   \"\\n\\nUPA IRC\\nWhat about 19:00 UTC?  e  | ταλκ \"   normal\n",
       "06e7f93938ad9e72  \"\\nI've re-added your information, together wi...   normal\n",
       "4a5e851879fdd674  \"\\nI'm not an elitist, I'm just spreading the ...   normal\n",
       "ff39db4975a78363  \"\\n\\nIt is not listed on this European list as...   normal\n",
       "73cc03c5e157ce86                        You made a mistake you ass.  obscene\n",
       "ca0891e20b7bbd66  Lol dynamic IP. Just you try to stop me! 82.13...   normal\n",
       "b890cc6153e51480  \"Thanks for trying to fix Neil Steinberg. I ju...   normal"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxicity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dd2dcd01f0536e53</th>\n",
       "      <td>\"\\nEdit:  Please stated the basis for your acc...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b20b1e8381e306f8</th>\n",
       "      <td>Wikipedia is a repetition of the old joke that...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54a933831401b9b2</th>\n",
       "      <td>And the same in Serbian:ђе and Croatian:če and...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7a4575dba88d9f3</th>\n",
       "      <td>\"\\n\\n Congrats... You gave an awesome answer i...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452d5cc2ccd1d611</th>\n",
       "      <td>Wiki users controlling information \\n\\nDeliber...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e13de405c4f2ae03</th>\n",
       "      <td>The name Tajik no more implies a lack of conne...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a7b33257178c17a7</th>\n",
       "      <td>Updated? \\n\\nAccording to this website (http:/...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d6093ce71ecbb577</th>\n",
       "      <td>\"\\n\\nIt is incorrect the way it sounds now. Th...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7e5f6eafd2cd3147</th>\n",
       "      <td>Template:Geobox coor \\n\\nHi, I had to do an em...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c665f590918ecce3</th>\n",
       "      <td>ADDITIONAL EVIDENCE TO ABOVE:  One of the coll...</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       comment_text toxicity\n",
       "id                                                                          \n",
       "dd2dcd01f0536e53  \"\\nEdit:  Please stated the basis for your acc...   normal\n",
       "b20b1e8381e306f8  Wikipedia is a repetition of the old joke that...   normal\n",
       "54a933831401b9b2  And the same in Serbian:ђе and Croatian:če and...   normal\n",
       "e7a4575dba88d9f3  \"\\n\\n Congrats... You gave an awesome answer i...   normal\n",
       "452d5cc2ccd1d611  Wiki users controlling information \\n\\nDeliber...   normal\n",
       "e13de405c4f2ae03  The name Tajik no more implies a lack of conne...   normal\n",
       "a7b33257178c17a7  Updated? \\n\\nAccording to this website (http:/...   normal\n",
       "d6093ce71ecbb577  \"\\n\\nIt is incorrect the way it sounds now. Th...   normal\n",
       "7e5f6eafd2cd3147  Template:Geobox coor \\n\\nHi, I had to do an em...   normal\n",
       "c665f590918ecce3  ADDITIONAL EVIDENCE TO ABOVE:  One of the coll...   normal"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data files include a column *comment_text* with the text we must classify, and an additional columns with the kind of toxicity that is presents in a comment: *toxic*, *severe_toxic*, *obscene*, *threat*, *insult* and *identity_hate*, or *normal* if the text contains no toxicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducing the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow faster experimenting, we will only use a portion of the data. Note that reducing the training data will result in worse model performance, and reducing the test data will result in a poorer estimate of the performance of the model. If you want to obtain the best results with the best confidence, do not run the following cell. But be prepared for a very, VERY long training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training patterns before reduction: 119678\n",
      "Training patterns after reduction:  1196\n",
      "Test patterns before reduction: 39893\n",
      "Test patterns after reduction:  398\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f\"Training patterns before reduction: {len(train)}\")\n",
    "train = train.sample(int(len(train)/100), random_state=12345)\n",
    "print(f\"Training patterns after reduction:  {len(train)}\")\n",
    "\n",
    "print(f\"Test patterns before reduction: {len(test)}\")\n",
    "test = test.sample(int(len(test)/100), random_state=12345)\n",
    "print(f\"Test patterns after reduction:  {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = train[\"comment_text\"].values\n",
    "X_test = test[\"comment_text\"].values\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train[\"toxicity\"].values)\n",
    "y_test = label_encoder.transform(test[\"toxicity\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convenient library to make use of Transformer-based language models is... [Transformers](https://github.com/huggingface/transformers)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/transformers_logo_name.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers provides implementations of many language models like BERT, GPT-2 and many more. It also allows to make use of pre-trained versions of these models, thus saving a lot of time when solving practical problems.\n",
    "\n",
    "Let's start by importing an AutoConfig object, which allows us to specify the configuration details for a language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/alvaro.barbero/anaconda3/envs/deeplearning-labs-gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the provided models, in this notebook we will make use of DistilBERT, a distilled version of BERT that can obtain good accuracies while keeping the model size small. We will use the configuration to tell Transformers we want to use a pretrained version of DistilBERT, trained on a dataset of uncased data, since case might not be important for the problem at hand. We also need to specify that we will use this pre-trained model to solve a classification problem with a specific number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'distilbert-base-uncased'\n",
    "num_labels = len(set(y_train))\n",
    "config = AutoConfig.from_pretrained(pretrained_model, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check out the resultant configuration, which contains all the model parameters, like dropout rates, embeddings sizes, and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertConfig {\n",
       "  \"activation\": \"gelu\",\n",
       "  \"architectures\": [\n",
       "    \"DistilBertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"dim\": 768,\n",
       "  \"dropout\": 0.1,\n",
       "  \"hidden_dim\": 3072,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\",\n",
       "    \"3\": \"LABEL_3\",\n",
       "    \"4\": \"LABEL_4\",\n",
       "    \"5\": \"LABEL_5\",\n",
       "    \"6\": \"LABEL_6\"\n",
       "  },\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2,\n",
       "    \"LABEL_3\": 3,\n",
       "    \"LABEL_4\": 4,\n",
       "    \"LABEL_5\": 5,\n",
       "    \"LABEL_6\": 6\n",
       "  },\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"distilbert\",\n",
       "  \"n_heads\": 12,\n",
       "  \"n_layers\": 6,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"qa_dropout\": 0.1,\n",
       "  \"seq_classif_dropout\": 0.2,\n",
       "  \"sinusoidal_pos_embds\": false,\n",
       "  \"tie_weights_\": true,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in a language model pipeline is to tokenize the data. We can do so using an AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will load a particular tokenizer: the one used for training DistilBERT. This tokenizer is pre-trained with an uncased dataset, following the pattern we specifyied in the configuration above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the tokenizer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'long', 'trip', 'to', 'mor', '##dor']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"A long trip to Mordor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most BERT models use a word-pieces tokenizer, dividing text into tokens that might represent a whole word, or a part of a word if the word is not common in the language. Also, since we are using an uncased model, the tokenizer maps all words to lower case.\n",
    "\n",
    "Equivalently, we can also ask the tokenizer to transform the text to a list of dictionary ids, plus other lists of indexes required by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1037, 2146, 4440, 2000, 22822, 7983, 102]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"A long trip to Mordor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This encoding also adds the special tokens `[CLS]` and `[SEP]` required by BERT models at the beginning and end of each text. We can check that out as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 -> [CLS]\n",
      "1037 -> a\n",
      "2146 -> long\n",
      "4440 -> trip\n",
      "2000 -> to\n",
      "22822 -> mor\n",
      "7983 -> ##dor\n",
      "102 -> [SEP]\n"
     ]
    }
   ],
   "source": [
    "for token_id in tokenizer.encode(\"A long trip to Mordor\"):\n",
    "    print(f'{token_id} -> {tokenizer.decode([token_id])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to tokenize more than one text we can use the `batch_encode_plus` function. We can configure this function to make sure that every encoded text has the same length, which will we useful when working in batches on the GPU. In  the following example we will use a common length of 10, which manages to cover all tokens in every one of these sample texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1037, 2146, 4440, 2000, 22822, 7983, 102, 0, 0], [101, 2256, 2568, 1037, 2712, 102, 0, 0, 0, 0], [101, 26661, 15750, 2003, 1996, 2203, 1997, 2422, 102, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"A long trip to Mordor\", \n",
    "    \"Our mind a sea\",\n",
    "    \"Mabuka is the end of light\"\n",
    "]\n",
    "\n",
    "tokenizer.batch_encode_plus(texts, max_length=10, pad_to_max_length=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_encode_plus` returns a dictionary with three entries:\n",
    "\n",
    "* `input_ids`: the ids of the tokens encoding each of the texts.\n",
    "* `attention_mask`: 0/1 indicators telling whether the attention layers should consider this token in the mixings or not. Padding tokens always get a 0 value in the mask.\n",
    "* (optional) `token_type_ids`: for language models that learn with pairs of sentences, 0/1 indicators telling the sentence to which each token belongs.\n",
    "\n",
    "All the returned elements will be necessary as inputs to the language model. Since DistilBERT does not learn from pairs of sentences, the `token_type_ids` entry is not returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also ask the `batch_encode_plus` function to produce Tensorflow or Pytorch tensors instead of python lists. For instance, to obtain Pytorch tensors we will do as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1037,  2146,  4440,  2000, 22822,  7983,   102,     0,     0],\n",
       "        [  101,  2256,  2568,  1037,  2712,   102,     0,     0,     0,     0],\n",
       "        [  101, 26661, 15750,  2003,  1996,  2203,  1997,  2422,   102,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus(texts, max_length=10, pad_to_max_length=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned structure is the same, but now each entry is a Pytorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what would be the ideal maximum length for encoding our texts? BERT accepts inputs texts as long as 512 tokens, but using always this maximum length will result in slow training times. We can try tokenizing all texts without length limitation and study the distribution of text lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
    "\n",
    "***\n",
    "\n",
    "<font color=#ad3e26>\n",
    "    Perform the following tasks to find an appropriate maximum length to use in the encoding:\n",
    "    <ul>\n",
    "        <li>Encode all texts in X_train using batch_encode_plus, but don't specify any maximum length, padding or tensor options.</li>\n",
    "        <li>Create a list with the lengths of each one of the indices vectors.</li>\n",
    "        <li>Find the value of 90% percentile of the lengths distribution. Tip: use the <a href=https://docs.scipy.org/doc/numpy/reference/generated/numpy.quantile.html>numpy quantile function</a>. Save this value as an integer number into a variable named <b>maxlength</b>.</li>\n",
    "    </ul>\n",
    "</font>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (548 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (592 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (647 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (630 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (600 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (607 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1035 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (528 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (799 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1009 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (915 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (650 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (896 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (648 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (963 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (625 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (514 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1125 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1074 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (930 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (565 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2273 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1022 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (964 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "219"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### INSERT YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "encoded = tokenizer.batch_encode_plus(X_train)\n",
    "lenghts = [len(x) for x in encoded[\"input_ids\"]]\n",
    "maxlength = int(np.quantile(lenghts, 0.9))\n",
    "maxlength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this maximum length later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DistilBERT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to explore the DistilBERT model. Let's load the pre-trained version of DistilBERT using an AutoModel class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "distilbert = AutoModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pre-trained version contains the \"body\" of the model, which can receive a sequence of tokens and produce the \"contextualized\" embeddings for each one of those tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/bert-encoders-input.png\">\n",
    "<div align=\"right\">Image credit: <a href=\"http://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try this with a batch of one text of the training data, but remember we first need to transform it through the tokenizer and obtain Pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  1000,  3835,  2862,   999,  1027,  1027, 19394,  2015,  2006,\n",
       "          4526,  2862,  1997, 27672,  6244,  5231,  1999,  1996,  2142,  2163,\n",
       "          1010,  2029,  2038,  2000,  2022,  2028,  1997,  1996,  6493,  2862,\n",
       "          1045,  2031,  2412,  2464,  2006,  2019,  3988,  3720,  4325,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = tokenizer.batch_encode_plus(X_train[0:1], max_length=40, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can input the tensor into the DistilBERT model. A convenient way to input all three tensors into the model is using the unpacking operator `**`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = distilbert(**sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Transformers model always returns a tuple which might contain several pieces of information. In the case of DistilBERT, only a single object is returned, which is a pytorch tensor containing the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape torch.Size([1, 40])\n",
      "Input tensor values tensor([[  101,  1000,  3835,  2862,   999,  1027,  1027, 19394,  2015,  2006,\n",
      "          4526,  2862,  1997, 27672,  6244,  5231,  1999,  1996,  2142,  2163,\n",
      "          1010,  2029,  2038,  2000,  2022,  2028,  1997,  1996,  6493,  2862,\n",
      "          1045,  2031,  2412,  2464,  2006,  2019,  3988,  3720,  4325,   102]])\n",
      "DistilBERT embeddings shape torch.Size([1, 40, 768])\n",
      "DistilBERT embeddings values tensor([[[ 0.0868,  0.1375, -0.1329,  ...,  0.0667,  0.4648,  0.4608],\n",
      "         [ 0.3281,  0.1915,  0.0478,  ...,  0.1433,  0.4600,  0.2365],\n",
      "         [ 0.3558,  0.1741,  0.4733,  ...,  0.1318,  0.2100,  0.1237],\n",
      "         ...,\n",
      "         [ 0.4565, -0.1248,  0.2463,  ...,  0.1870,  0.0948, -0.0672],\n",
      "         [ 0.1072, -0.0313,  0.0774,  ...,  0.1644,  0.2107, -0.2898],\n",
      "         [ 0.7191,  0.2547, -0.1020,  ...,  0.2076, -0.6422, -0.2012]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "embeddings = outputs[0]\n",
    "print(f\"Input tensor shape {sample['input_ids'].shape}\")\n",
    "print(f\"Input tensor values {sample['input_ids']}\")\n",
    "print(f\"DistilBERT embeddings shape {embeddings.shape}\")\n",
    "print(f\"DistilBERT embeddings values {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DistilBERT returns an embedding vector of 768 numbers for each input token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it is tempting to use these embeddings as features for the toxic classification task, this approach does not generally give good results. Instead, it is advisable to add a classification \"head\" to the model, growing out of the embedding produced for the `[CLS]` special token, and fine-tune the whole model to the task through back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://jalammar.github.io/images/bert-classifier.png\">\n",
    "<div align=\"right\">Image credit: <a href=\"http://jalammar.github.io/illustrated-bert/\">The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformers library can prepare all of this for us, by loading a version of DistilBERT with a Sequence Classification head. We will provide the configuration we prepared above, so the the classification head produces as many outputs as classes in our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "distilbert_classification = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing for fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning a Transformers model is not as simple as fitting scikit-learn or Keras model: we will need to provide all the details on how to batch the data, as well as other details on the training procedure. To easen this task, we will first prepare some useful functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, fine-tuning a language model on the CPU is not a good idea. We will be better off using a GPU. To do so, we first need to identify the computing device. The code below checks if a GPU is available in the system, and if so, prepares a Torch device to send the calculations there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above should print `device(type='cuda')` if a GPU was found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if your machine has several GPUs, by default the first one will be used, but the underlying deep learning library (Tensorflow or Pytorch) might lock the memory from all GPUs in the system. To constrain this notebook to use a specific GPU, run jupyter notebook as \n",
    "\n",
    "   `CUDA_VISIBLE_DEVICES=X jupyter notebook`\n",
    "   \n",
    "where `X` is the number of the GPU you want to use. You can check your available GPUs and their usage with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 11 13:51:53 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.64       Driver Version: 430.64       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K20m          Off  | 00000000:85:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    47W / 225W |    232MiB /  4743MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K20m          Off  | 00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P0    47W / 225W |     69MiB /  4743MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla K20m          Off  | 00000000:89:00.0 Off |                    0 |\r\n",
      "| N/A   37C    P8    15W / 225W |     11MiB /  4743MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla K20m          Off  | 00000000:8A:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P8    14W / 225W |      0MiB /  4743MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     12533      C   ... -ep_eval 3 -l_name liga_2805 -w_u True   221MiB |\r\n",
      "|    1     12533      C   ... -ep_eval 3 -l_name liga_2805 -w_u True    58MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any dataset used in Transformers must follow one contraint: the dataset must be an iterable of samples. But what is a sample? A text together with the outputs we expect our model to generate for it. In our classification problem each sample will be a text together with its class, so a natural way to organize a sample is the forma a tuple (text, class). We will prepare out training and test data in this format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = list(zip(X_train, y_train))\n",
    "eval_dataset = list(zip(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will need a <b>collate class</b> that receives an part of a dataset (iterable of samples), and performs all the tokenization and encoding procedure to obtain a Torch tensor in GPU. This class should inherit from the Transformers `DataCollator` class, and implement a `collate_batch` method that receives an iterable of samples and returns an encoded batch. Here we provide an implementation for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollator\n",
    "\n",
    "class TextClassificationCollator(DataCollator):\n",
    "    \"\"\"Data collator for a text classification problem\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        \"\"\"Initializes the collator with a tokenizer and a maximum document length (in tokens)\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def encode_texts(self, texts):\n",
    "        \"\"\"Transforms an iterable of texts into a dictionary of model input tensors, stored in the GPU\"\"\"\n",
    "        # Tokenize and encode texts as tensors, with maximum length\n",
    "        tensors = self.tokenizer.batch_encode_plus(\n",
    "            texts, \n",
    "            max_length=self.max_length, \n",
    "            pad_to_max_length=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # Move tensors to GPU\n",
    "        for key in tensors:\n",
    "            tensors[key] = tensors[key].to(device)\n",
    "        return tensors\n",
    "    \n",
    "    def collate_batch(self, patterns):\n",
    "        \"\"\"Collate a batch of patterns\n",
    "        \n",
    "        Arguments:\n",
    "            - patterns: iterable of tuples in the form (text, class)\n",
    "            \n",
    "        Output: dictionary of torch tensors ready for model input\n",
    "        \"\"\"\n",
    "        # Split texts and classes from the input list of tuples\n",
    "        train_idx, targets = zip(*patterns)\n",
    "        # Encode inputs\n",
    "        input_tensors = self.encode_texts(train_idx)\n",
    "        # Transform class labels to a tensor in GPU\n",
    "        Y = torch.tensor(targets).long().to(device)\n",
    "        # Return batch as a dictionary wikth all the inputs tensors and the labels\n",
    "        batch = {**input_tensors, \"labels\": Y}\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the collator class defined, we will create an instance for the particular tokenizer and maximum sequence length we have chosen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "collator = TextClassificationCollator(tokenizer, maxlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is creating a `TrainingArguments` object. This object allows us to specify the training procedure details. For this notebook we will use a batch size of 32, which should fit into a small GPU. We will also train the model only 1 epoch over the training data, to allow us to check the results quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/toxic_model\",  # Folder in which to save the trained model\n",
    "    overwrite_output_dir=True,  # Whether to overwrite previous models found in the output folder\n",
    "    per_gpu_train_batch_size=16,  # batch size during training\n",
    "    per_gpu_eval_batch_size=128,  # batch size during evaluation (prediction)\n",
    "    num_train_epochs=1,  # Model training epochs\n",
    "    logging_steps=25,  # After how many training steps (batches) a log message showing progress will be printed\n",
    "    save_steps=25  # After how many training steps (batches) the model will be checkpointed to disk\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step of the preparation, is to create a `Trainer` object. This is the object that performs the actual training, and will need to receive all the information we prepared above, which is:\n",
    "\n",
    "* The model to be fine-tuned\n",
    "* The `TrainingArguments` object we prepared above\n",
    "* Training and evaluation datasets\n",
    "* The DataCollator object that will batch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=distilbert_classification,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is ready for training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the work above is done, training is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a212d5b78d47178c5ee9847ea8d900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=1.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3753daf8a5483187bb8142cd69bc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=75.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"loss\": 0.8196162295341491, \"learning_rate\": 3.3333333333333335e-05, \"epoch\": 0.3333333333333333, \"step\": 25}\n",
      "{\"loss\": 0.3887899678945541, \"learning_rate\": 1.6666666666666667e-05, \"epoch\": 0.6666666666666666, \"step\": 50}\n",
      "{\"loss\": 0.41151548027992246, \"learning_rate\": 0.0, \"epoch\": 1.0, \"step\": 75}\n",
      "\n",
      "\n",
      "CPU times: user 37.1 s, sys: 17.1 s, total: 54.2 s\n",
      "Wall time: 1min 34s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=75, training_loss=0.539973892569542)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained we can obtain the model predictions with the `predict` method of the `Trainer` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148d3289499145dfb06ad14aec2403e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Prediction', max=4.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in a `ForSequenceClassification` model each prediction is a list of **unnormalized** probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7314742 ,  0.03136623,  4.298269  , -0.98842025, -1.3786166 ,\n",
       "       -1.3473176 ,  0.37629357], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain actual probabilities we need to apply a `softmax` function, which enforces each probability to take a value in the range `[0, 1]`, and also that all probabilities sum up to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "probs = softmax(preds.predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check now that indeed the number we have look like probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00621493, 0.01332705, 0.95022416, 0.00480669, 0.00325377,\n",
       "       0.00335721, 0.01881629], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these probabilities to compute any standard metric from scikit-learn. For instance, ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC score 0.9055487893001456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(\"AUC score\", roc_auc_score(y_test, probs, multi_class='ovr'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
